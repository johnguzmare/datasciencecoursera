

swirl Lesson 1: Residual Variation
========

 Residual Variation. (Slides for this and other Data Science courses may be found at github
 https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip
 file and viewed locally. This lesson corresponds to Regression_Models/01_06_residualVariation. Galton data is
 from John Verzani's website, http://wiener.math.csi.cuny.edu/UsingR/)

 As shown in the slides, residuals are useful for indicating how well data points fit a statistical model. They
 "can be thought of as the outcome (Y) with the linear association of the predictor (X) removed. One
 differentiates residual variation (variation after removing the predictor) from systematic variation (variation
 explained by the regression model)."

 It can also be shown that, given a model, the maximum likelihood estimate of the variance of the random error is
 the average squared residual. However, since our linear model with one predictor requires two parameters we have
 only (n-2) degrees of freedom. Therefore, to calculate an "average" squared residual to estimate the variance we
 use the formula 1/(n-2) * (the sum of the squared residuals). If we divided the sum of the squared residuals by
 n, instead of n-2, the result would give a biased estimate.

 To see this we'll use our favorite Galton height data. First regenerate the regression line and call it fit. Use
 the R function lm and recall that by default its first argument is a formula such as "child ~ parent" and its
 second is the dataset, in this case galton.
```r
> fit <- lm(child ~ parent,data = galton)
```
First, we'll use the residuals (fit$residuals) of our model to estimate the standard deviation (sigma) of the
 error. We've already defined n for you as the number of points in Galton's dataset (928).

 Calculate the sum of the squared residuals divided by the quantity (n-2).  Then take the square root.
```r
> sqrt(sum(fit$residuals^2) / (n - 2))
[1] 2.238547
```
Now look at the "sigma" portion of the summary of fit, "summary(fit)$sigma".
```r
> summary(fit)$sigma
[1] 2.238547

> sqrt(deviance(fit)/(n-2))
[1] 2.238547
```

Total Variation = Residual Variation + Regression Variation


 The term R^2 represents the percent of total variation described by the model, the regression variation (the term
 we didn't ask about in the preceding multiple choice questions). Also, since it is a percent we need a ratio or
 fraction of sums of squares. Let's do this now for our Galton data.

We'll start with easy steps. Calculate the mean of the children's heights and store it in a variable called mu.
 Recall that we reference the childrens' heights with the expression 'galton$child' and the parents' heights with
 the expression 'galton$parent'.
```r
> mu <- mean(galton$child)
```
Recall that centering data means subtracting the mean from each data point. Now calculate the sum of the squares
 of the centered children's heights and store the result in a variable called sTot. This represents the Total
 Variation of the data.
```r
> sTot <- sum((galton$child-mu)^2)
```
Now create the variable sRes. Use the R function deviance to calculate the sum of the squares of the residuals.
 These are the distances between the children's heights and the regression line. This represents the Residual
 Variation.
```r
> sRes <- deviance(fit)
```
Finally, the ratio sRes/sTot represents the percent of total variation contributed by the residuals. To find the
 percent contributed by the model, i.e., the regression variation, subtract the fraction sRes/sTot from 1.  This
 is the value R^2.
```r
> 1- sRes/sTot
[1] 0.2104629
```
For fun you can compare your result to the values shown in summary(fit)$r.squared to see if it looks familiar. Do
 this now.
```r
> summary(fit)$r.squared
[1] 0.2104629
```
compute the square of the correlation of the galton data, the children and parents. Use the R function cor.
```r
> cor(galton$parent,galton$child)^2
[1] 0.2104629
```
We'll now summarize useful facts about R^2. It is the percentage of variation explained by the regression model.
 As a percentage it is between 0 and 1. It also equals the sample correlation squared. However, R^2 doesn't tell
 the whole story.




