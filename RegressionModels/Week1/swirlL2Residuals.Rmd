
Practice Programming Assignment: swirl Lesson : Residuals.
=========================

 (Slides for this and other Data Science courses may be found at github
  https://github.com/DataScienceSpecialization/courses. If you care to use them,
  they must be downloaded as a zip file and viewed locally. This lesson corresponds
  to Regression_Models/01_03_ols. Galton data is from John Verzani's website,
  http://wiener.math.csi.cuny.edu/UsingR/)

  This lesson will focus on the residuals, the distances between the actual
  children's heights and the estimates given by the regression line. Since all
  lines are characterized by two parameters, a slope and an intercept, we'll use
  the least squares criteria to provide two equations in two unknowns so we can
  solve for these parameters, the slope and intercept

  The first equation says that the "errors" in our estimates, the residuals, have
  mean zero. In other words, the residuals are "balanced" among the data points;
  they're just as likely to be positive as negative. The second equation says that
  our residuals must be uncorrelated with our predictors, the parents’ height. This
  makes sense - if the residuals and predictors were correlated then you could make
  a better prediction and reduce the distances (residuals) between the actual
  outcomes and the predictions.

  We'll demonstrate these concepts now. First regenerate the regression line and
  call it fit. Use the R function lm. Recall that by default its first argument is
  a formula such as "child ~ parent" and its second is the dataset, in this case
  galton.


![alt text](Rplot.png)


We'll demonstrate these concepts now. First regenerate the regression line and
  call it fit. Use the R function lm. Recall that by default its first argument is
  a formula such as "child ~ parent" and its second is the dataset, in this case
  galton.

```r
> fit <- lm(child ~ parent,galton)
```

Now we'll examine fit to see its slope and intercept. The residuals we're
  interested in are stored in the 928-long vector fit$residuals. If you type
  fit$residuals you'll see a lot of numbers scroll by which isn't very useful;
  however if you type "summary(fit)" you will see a more concise display of the
  regression data. Do this now.

```r
> summary(fit)

Call:
lm(formula = child ~ parent, data = galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(> t )    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

First check the mean of fit$residuals to see if it's close to 0.
```r
> mean(fit$residuals)
[1] 8.935041e-17
```
Now check the correlation between the residuals and the predictors. Type
  "cov(fit$residuals, galton$parent)" to see if it's close to 0.
```r
cov(fit$residuals, galton$parent)
[1] -9.487809e-15
```
As shown algebraically in the slides, the equations for the intercept and slope
  are found by supposing a change is made to the intercept and slope. Squaring out
  the resulting expressions produces three summations. The first sum is the
  original term squared, before the slope and intercept were changed. The third sum
  totals the squared changes themselves. For instance, if we had changed fit’s
  intercept by adding 2, the third sum would be the total of 928 4’s. The middle
  sum is guaranteed to be zero precisely when the two equations (the conditions on
  the residuals) are satisfied.

 Let "mch" represent the mean of the galton childrens' heights and "mph" the mean
  of the galton parents' heights. Let "ic" and "slope" represent the intercept and
  slope of the regression line respectively. As shown in the slides and past
  lessons, the point (mph,mch) lies on the regression line. This means

1: mph = ic + slope*mch
2: mch = ic + slope*mph
3: I haven't the slightest idea.

Selection: 2

The function sqe calculates the sum of the squared residuals, the differences
  between the actual children's heights and the estimated heights specified by the
  line defined by the given parameters (slope and intercept).  R provides the
  function deviance to do exactly this using a fitted model (e.g., fit) as its
  argument. However, we provide sqe because we'll use it to test regression lines
  different from fit.

We'll see that when we vary or tweak the slope and intercept values of the
  regression line which are stored in fit$coef, the resulting squared residuals are
  approximately equal to the sum of two sums of squares - that of the original
  regression residuals and that of the tweaks themselves. More precisely, up to
  numerical error,

sqe(ols.slope+sl,ols.intercept+ic) == deviance(fit) + sum(est(sl,ic)ˆ2 )

Equivalently, sqe(ols.slope+sl,ols.intercept+ic) == sqe(ols.slope, ols.intercept)
  + sum(est(sl,ic)ˆ2 )

The left side of the equation represents the squared residuals of a new line, the
  "tweaked" regression line. The terms "sl" and "ic" represent the variations in
  the slope and intercept respectively. The right side has two terms. The first
  represents the squared residuals of the original regression line and the second
  is the sum of squares of the variations themselves.

We'll demonstrate this now. First extract the intercept from fit$coef and put it
  in a variable called ols.ic . The intercept is the first element in the fit$coef
  vector, that is fit$coef[1].
```r
> ols.ic <- fit$coef[1]
```
 Now extract the slope from fit$coef and put it in the variable ols.slope; the slope is the second element in
 the fit$coef vector, fit$coef[2].
```r
> ols.slope <- fit$coef[2]
```
 Now we'll show you some R code which generates the left and right sides of this equation.  Take a moment to
 look it over. We've formed two 6-long vectors of variations, one for the slope and one for the intercept. Then
 we have two "for" loops to generate the two sides of the equation.

```r
#Here are the vectors of variations or tweaks
sltweak <- c(.01, .02, .03, -.01, -.02, -.03) #one for the slope
ictweak <- c(.1, .2, .3, -.1, -.2, -.3)  #one for the intercept
lhs <- numeric()
rhs <- numeric()
#left side of eqn is the sum of squares of residuals of the tweaked regression line
for (n in 1:6) lhs[n] <- sqe(ols.slope+sltweak[n],ols.ic+ictweak[n])
#right side of eqn is the sum of squares of original residuals + sum of squares of two tweaks
for (n in 1:6) rhs[n] <- sqe(ols.slope,ols.ic) + sum(est(sltweak[n],ictweak[n])^2)
```

Subtract the right side, the vector rhs, from the left, the vector lhs, to see the relationship between them.
 You should get a vector of very small, almost 0, numbers.

```r
> lhs-rhs
[1]  5.366019e-11  9.367795e-11  1.546141e-10 -4.365575e-11 -9.458745e-11 -1.273293e-10
```

You could also use the R function all.equal with lhs and rhs as arguments to test for equality. Try it now.

```r
> all.equal(lhs,rhs)
[1] TRUE
```

Now we'll show that the variance in the children's heights is the sum of the variance in the OLS estimates and
 the variance in the OLS residuals. First use the R function var to calculate the variance in the children's
 heights and store it in the variable varChild.

```r
> varChild <- var(galton$child)
```

 Remember that we've calculated the residuals and they're stored in fit$residuals. Use the R function var to
 calculate the variance in these residuals now and store it in the variable varRes.

```r
varRes <- var(fit$residuals)
```

Recall that the function "est" calculates the estimates (y-coordinates) of values along the regression line
 defined by the variables "ols.slope" and "ols.ic". Compute the variance in the estimates and store it in the
 variable varEst.

```r
varEst <- var(est(ols.slope, ols.ic))
```
Now use the function all.equal to compare varChild and the sum of varRes and varEst.

```r
> all.equal(varChild,varEst+varRes)
[1] TRUE
```

 Since variances are sums of squares (and hence always positive), this equation which we've just demonstrated,
 var(data)=var(estimate)+var(residuals), shows that the variance of the estimate is ALWAYS less than the
 variance of the data.


Since var(data)=var(estimate)+var(residuals) and variances are always positive, the variance of residuals

1: is less than the variance of data
2: is unknown without actual data
3: is greater than the variance of data

Selection: 1

 The two properties of the residuals we've emphasized here can be applied to datasets which have multiple
 predictors. In this lesson we've loaded the dataset attenu which gives data for 23 earthquakes in California.
 Accelerations are estimated based on two predictors, distance and magnitude.

```r
> head(attenu)
  event mag station dist accel
1     1 7.0     117   12 0.359
2     2 7.4    1083  148 0.014
3     2 7.4    1095   42 0.196
4     2 7.4     283   85 0.135
5     2 7.4     135  107 0.062
6     2 7.4     475  109 0.054
```

Generate the regression line for this data. Type efit <- lm(accel ~ mag+dist, attenu) at the R prompt.

```r
> efit <- lm(accel ~ mag+dist, attenu) 
```

Verify the mean of the residuals is 0.

```r
> mean(efit$residuals)
[1] -2.686528e-18
```
Using the R function cov verify the residuals are uncorrelated with the magnitude predictor, attenu$mag.

```r
> cov(efit$residuals, attenu$mag)
[1] -6.394247e-18
```

Using the R function cov verify the residuals are uncorrelated with the distance predictor, attenu$dist.
```r
> cov(efit$residuals, attenu$dist)
[1] -7.698539e-17
```

